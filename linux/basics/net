I- Hardware
#
procfs : /proc/net/dev
#
ifconfig                      = display all up net dev with statistics(*)
          <net_dev>           = display net_dev stats
          -a                  = display all  net dev up/down 
          -s                  = print stat summery
          txqueuelen <length> = TX queue length (E.g: 10000 on 10 GbE )

ethtool <net_dev>             = print device current settings
        -s OPTIONS <net_dev>  = change device settings
        -k <net_dev>          = show offload features 
        -K <net_dev>          = change offload features  
        -S <net_dev>          = query device statistics
        -G <net_dev> <value>  = set ring buffer number
        -i <net_dev>          = show device driver name

# ethtool Options
speed <value> duplex half|full autoneg on|off advertise <hex_code> wol p|u|m|b|a|g|s|d 
  
speed     : network device speed in Mb/s
duplex    : set full or half duplex mode
autoneg   : negotiate network for highest speed
advertise : advertise autonegociate speed limit 
wol       : wake on lan (p:PHY,a:ARP,d:Off)

Note: ethtool options can be set via network device file /etc/sysconfig/network-scripts/ifcfg-* 
using ETHTOOL QPTS="OPTIONS"

#
ip <options> link                 : for all net dev
ip <options> link show <net_dev>  : only for net_dev
                 
options:    -s : print network statistics 
            -a : all network devices
            -d : more details 

Note: "ip a" (shorcut for ip address ) show all network interfaces with ip addresses 

II- Tuning

1) Socket and TCP Buffers
#Documentation available in /usr/share/doc/kernel-doc-<version>/Documentation/sysctl/net.txt
#Documentation available in /usr/share/doc/kernel-doc-<version>/Documentation/networking/ip-sysctl.txt

# TCP Buffer Size Calculation
 
BDP (Bandwidth Delay Product) = Bandwidth * RTT (Round-Time Trip)

# Core networking maximum socket receive/send (read/write) buffers
net.core.wmem max  = <BDP_in_bytes>
net.core.rmem_max  = <BDP_in_bytes>

Note: net.core.wmem_default and net.core.rmem_default are the default values
# TCP receive/send (read/write) Buffer tunning
min     : minimum memory in bytes for each tcp connection 
default : default memory in bytes for each tcp connection
max     : maximum memory in bytes for each tcp connection

net.ipv4.tcp_rmem = "min default max"
net.ipv4.tcp_wmem = "min default max"

Note: net.ipv4.tcp_mem and net.ipv4.udp_mem are systemwide memory limits for TCP and UDP connection in Pages (defaults are fine)

2) Other Kernel 

i-Window Scaling 
#
Allow TCP window size (unacknowledged data) to grow in size (default limit 64 KiB, max 1 GiB)
 
net.ipv4.tcp_window_scaling = 1 enabled (default) 
                              0 disbaled

Note: window size needs to be >= BW * RTT

ii-Busy Polling :
# 
An interrupt is sent only when either a timer (polling) or a certain number of packets is reached , instead of interrupt for every packet.

# Specific Socket (with SO_BUSY_POLL set)
net.core.busy_poll = <polling_timer>        
#  globally
net.core.busy_read = <polling_timer>

polling_timer:  50  us for a small number of sockets 
                100 us for a large numbers of sockets

iii-Network Interface

Increase the size of the interface queue.

ifconfig <net_dev> txqueuelen 10000 (for 10GbE)

iv- Device Backlog
#
Length of the network device backlog queue, per CPU

net.core.netdev_max_backlog = 10000 ( for 10 GbE)

3) MTU (Maximum Transmission Unit)

default MTU is 1500 bytes can be set up to 9000 bytes (Jumbo Frames) in network device configuraton file :

in /etc/sysconfig/network-scripts/ifcfg-<net_dev> change MTU=<new_size> 


4) Network Scaling Techniques
#Documentation available in /usr/share/doc/kernel-doc-<version>/Documentation/networking/scaling.txt

Note: each receive queue has a separate IRQ associated with it.

i- Receive Side Scaling (RSS) : 
#
NICs with multiple queues distribute receive interrupts over CPUs , based on address and TCP port numbers.
Configure each queue is mapped to each CPU : /sys/class/net/<net_dev>/queues/rx-<queue_num>/rss_cpus (bitmask,default 00 disabled) 

ii- Receive Packet Steering (RPS) : 
#
Software RSS for NICs with no multiple queues support.(fewer hardware queues than CPUs)
Configure single queue to all CPUs : /sys/class/net/<net_dev>/queues/rx-<queue_num>/rps_cpus (bitmask,default 00 disabled)

Note: When RPS is disbaled packets are processed on the interrupting CPU

iii- Receive Flow Steering (RFS):

RPS with affinity for where the socket was last processed on-CPU, to improve CPU cache hit rates
Configure RFS : /proc/sys/net/core/rps_sock_flow_entries = 32768
                /sys/class/net/<dev>/queues/rx-<n>/rps_flow_cnt = rps_sock_flow_entries/<number_of_queues>

iv- Accelerated RFS : This achieves RFS in hardware, for
#
Packets are sent directly to a CPU that is local to the thread consuming the datais executing the application.(NIC hardware support)

v- Transmit Packet Steering (XPS) : 
#
For NICs with multiple transmit queues, this supports transmission by multiple CPUs to the queues.
Configure each queue is mapped to each CPU : /sys/class/net/<net_dev>/queues/tx-<queue_num>/xps_cpus (bitmask,default 00 disabled)

III- Link Aggregation

1) Teamd Runners
   
 broadcast    : team device transmits packets via all ports.
 roundrobin   : team device transmits packets in a round-robin fashion.
 activebackup : watches for link changes and selects active port to be used for data transfers.
 loadbalance  : monitors traffic and uses a hash function to try to reach a perfect balance
 lacp         : 802.3ad Link Aggregation Control Protocol



IV- Benchmark

1) qperf

On Server:
# listen on default port 19765
qperf

On Client
# Test TCP Bandwidth for 60 s
qperf -t 60 --use_bits_per_sec <server> tcp_bw
# Test TCP Latency
qperf -vvs <server> tcp_lat

2) iperf

On Server:
# listen on default port 5001, with buffer size of 128K instead of default 8 KB
iperf -s -l 128k

On Client:
iperf -c <server> -l 128k -P 2 -i 1 -t 60
#
-P 2: run in parallel mode with two client threads
-i 1: print interval summaries every second
-t 60: total duration of the test: 60 s


(*)
RX packets : packets has been received.
TX packets : packets has been transmitted.
errors     : errors when transmitting or receiving.
dropped    : dropped packets when transmitting orreceiving.
overruns   : times network device id not have enough buffer space to send or receive a packet.
frame      : low-level Ethernet frame errors.
carrier    : packets discarded because of link media failure (such as a faulty cable)
