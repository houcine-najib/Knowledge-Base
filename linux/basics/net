I- Hardware
#
procfs : /proc/net/dev
#
ifconfig                      = display all up net dev with statistics(*)
          <net_dev>           = display net_dev stats
          -a                  = display all  net dev up/down 
          -s                  = print stat summery
          txqueuelen <length> = TX queue length (E.g: 10000 on 10 GbE )

ethtool <net_dev>             = print device current settings
        -s OPTIONS <net_dev>  = change device settings
        -k <net_dev>          = show offload features 
        -K <net_dev>          = change offload features  
        -S <net_dev>          = query device statistics
        -G <net_dev> <value>  = set ring buffer number
        -i <net_dev>          = show device driver name

# ethtool Options
speed <value> duplex half|full autoneg on|off advertise <hex_code> wol p|u|m|b|a|g|s|d 
  
speed     : network device speed in Mb/s
duplex    : set full or half duplex mode
autoneg   : negotiate network for highest speed
advertise : advertise autonegociate speed limit 
wol       : wake on lan (p:PHY,a:ARP,d:Off)

Note: ethtool options can be set via network device file /etc/sysconfig/network-scripts/ifcfg-* 
using ETHTOOL QPTS="OPTIONS"

#
ip <options> link                 : for all net dev
ip <options> link show <net_dev>  : only for net_dev
                 
options:    -s : print network statistics 
            -a : all network devices
            -d : more details 

Note: "ip a" (shorcut for ip address ) show all network interfaces with ip addresses 

II- Tuning

1) Socket and TCP Buffers
#Documentation available in /usr/share/doc/kernel-doc-<version>/Documentation/sysctl/net.txt
#Documentation available in /usr/share/doc/kernel-doc-<version>/Documentation/networking/ip-sysctl.txt

# TCP Buffer Size Calculation
 
BDP (Bandwidth Delay Product) = Bandwidth * RTT (Round-Time Trip)

# Core networking maximum socket receive/send (read/write) buffers
net.core.wmem max  = <BDP_in_bytes>
net.core.rmem_max  = <BDP_in_bytes>

Note: net.core.wmem_default and net.core.rmem_default are the default values
# TCP receive/send (read/write) Buffer tunning
min     : minimum memory in bytes for each tcp connection 
default : default memory in bytes for each tcp connection
max     : maximum memory in bytes for each tcp connection

net.ipv4.tcp_rmem = "min default max"
net.ipv4.tcp_wmem = "min default max"

Note: net.ipv4.tcp_mem and net.ipv4.udp_mem are systemwide memory limits for TCP and UDP connection in Pages (defaults are fine)

2) Other Kernel 

i-Window Scaling 
#
Allow TCP window size (unacknowledged data) to grow in size (default limit 64 KiB, max 1 GiB)
 
net.ipv4.tcp_window_scaling = 1 enabled (default) 
                              0 disbaled

Note: window size needs to be >= BW * RTT

ii-Busy Polling :
# 
An interrupt is sent only when either a timer (polling) or a certain number of packets is reached , instead of interrupt for every packet.

# Specific Socket (with SO_BUSY_POLL set)
net.core.busy_poll = <polling_timer>        
#  globally
net.core.busy_read = <polling_timer>

polling_timer:  50  us for a small number of sockets 
                100 us for a large numbers of sockets

iii-Network Interface

Increase the size of the interface queue.

ifconfig <net_dev> txqueuelen 10000 (for 10GbE)

iv- Device Backlog
#
Length of the network device backlog queue, per CPU

net.core.netdev_max_backlog = 10000 ( for 10 GbE)

3) MTU (Maximum Transmission Unit)

default MTU is 1500 bytes can be set up to 9000 bytes (Jumbo Frames) in network device configuraton file :

in /etc/sysconfig/network-scripts/ifcfg-<net_dev> change MTU=<new_size> 


4) Network Scaling Techniques
#Documentation available in /usr/share/doc/kernel-doc-<version>/Documentation/networking/scaling.txt

Note: each receive queue has a separate IRQ associated with it.

i- Receive Side Scaling (RSS) : 
#
NICs with multiple queues distribute receive interrupts over CPUs , based on address and TCP port numbers.
Configure each queue is mapped to each CPU : /sys/class/net/<net_dev>/queues/rx-<queue_num>/rss_cpus (bitmask,default 00 disabled) 

ii- Receive Packet Steering (RPS) : 
#
Software RSS for NICs with no multiple queues support.(fewer hardware queues than CPUs)
Configure single queue to all CPUs : /sys/class/net/<net_dev>/queues/rx-<queue_num>/rps_cpus (bitmask,default 00 disabled)

Note: When RPS is disbaled packets are processed on the interrupting CPU

iii- Receive Flow Steering (RFS):

RPS with affinity for where the socket was last processed on-CPU, to improve CPU cache hit rates
Configure RFS : /proc/sys/net/core/rps_sock_flow_entries = 32768
                /sys/class/net/<dev>/queues/rx-<n>/rps_flow_cnt = rps_sock_flow_entries/<number_of_queues>

iv- Accelerated RFS : This achieves RFS in hardware, for
#
Packets are sent directly to a CPU that is local to the thread consuming the datais executing the application.(NIC hardware support)

v- Transmit Packet Steering (XPS) : 
#
For NICs with multiple transmit queues, this supports transmission by multiple CPUs to the queues.
Configure each queue is mapped to each CPU : /sys/class/net/<net_dev>/queues/tx-<queue_num>/xps_cpus (bitmask,default 00 disabled)

III- Link Aggregation

1) Teamd Runners
   
 broadcast    : team device transmits packets via all ports.
 roundrobin   : team device transmits packets in a round-robin fashion.
 activebackup : watches for link changes and selects active port to be used for data transfers.
 loadbalance  : monitors traffic and uses a hash function to try to reach a perfect balance
 lacp         : 802.3ad Link Aggregation Control Protocol

2) nmcli

i- Create new team interface:
#
nmcli con add type team con-name <team_name> ifname <team_ifname> team.config '<JSON_config>'

team_name   : team or connection name (if not specified, get derived from IFNAME)
team_ifname : team interface name 
JSON_CONFIG : JSON configuration string or file  E.g:'{"runner": {"name": "TYPE" } }' (see man teamd.conf)

Configuration file will be created : /etc/sysconfig/network-scripts/ifcfg-<team_name>

Note: Exmaples can be found in : /usr/share/doc/teamd-*/example_configs

i- View team interface
#
nmcli con show <team_name>
#
nmcli con show (to view all interfaces)

iii- Modify team interface
#
Adjust team configuration 
nmcli con mod <team_name>  team.config <team.config_or_JSON-config>

Assign ports interfaces
nmcli con add type team-slave con-name <team_port> ifname <port_ifname> master <team_name> 

Assign static ip address
by default the team  interface get its IP settings using DHCP
Step-1: nmcli con mod <team_name>  ipv4.addresses 1.2.3.4/24
Step-2: nmcli con mod <team_name>  ipv4.method manual

Bring team interface up
Method-1: nmcli con up <team_name>
Method-2: nmcli con up <team_port_1..n>

3) ifcfg files
#
Create /etc/sysconfig/network-scripts/ifcfg-<team_name>
E.g:

DEVICE=team0
DEVICETYPE=Team
ONBOOT=yes
BOOTPROTO=none
IPADDR=192.168.11.1
PREFIX=24
TEAM_CONFIG='{"runner": {"name": "activebackup"}, "link_watch": {"name":"ethtool"}}'

Create port ifcfg to be a member of team interface  /etc/sysconfig/network-scripts/ifcfg-<team_port>
E.g:
DEVICE=eth1
HWADDR=D4:85:64:01:46:9E
DEVICETYPE=TeamPort
ONBOOT=yes
TEAM_MASTER=team0
TEAM_PORT_CONFIG='{"prio":100}'

Bring team interface up
ifup <team_name>

Note: Exmaples can be found in : /usr/share/doc/teamd-*/example_ifcfg/*/

4) Monitor and Troubleshoot team interface
#
Display current state of team interface : teamdctl <team_name> state veiw -v
Dump   current state of team interface    : teamdctl <team_device> state dump
Dump   configuration of team interface    : teamdctl <team_device> config dump
Disply team ports    of team interface    : teamnl   <team_device>   ports
Get    activeport    of team interface    : teamnl   <team_device>   getoption activeport
Set    activeport    of team interface    : teamnl   <team_device>   setoption activeport <port_id>
View   available options                  : teamnl   <team_device>   options

IV- Benchmark

1) qperf

On Server:
# listen on default port 19765
qperf

On Client
# Test TCP Bandwidth for 60 s
qperf -t 60 --use_bits_per_sec <server> tcp_bw
# Test TCP Latency
qperf -vvs <server> tcp_lat

2) iperf

On Server:
# listen on default port 5001, with buffer size of 128K instead of default 8 KB
iperf -s -l 128k

On Client:
iperf -c <server> -l 128k -P 2 -i 1 -t 60
#
-P 2: run in parallel mode with two client threads
-i 1: print interval summaries every second
-t 60: total duration of the test: 60 s

(*)
RX packets : packets has been received.
TX packets : packets has been transmitted.
errors     : errors when transmitting or receiving.
dropped    : dropped packets when transmitting orreceiving.
overruns   : times network device id not have enough buffer space to send or receive a packet.
frame      : low-level Ethernet frame errors.
carrier    : packets discarded because of link media failure (such as a faulty cable)
